{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# House Prices 7: Gradient Boosting\n",
    "Gradient boosting machines have had a long history of success in machine learning competitions. They are a powerful tool for regression and classification problems. In this notebook, I will try:\n",
    "1. `XGBoost`\n",
    "1. `LightGBM`\n",
    "1. `CatBoost`\n",
    "\n",
    "I'll use `Optuna` to optimize the hyperparameters of the models.\n",
    "\n",
    "<!-- I'll also use `ELI5` to understand and compare the importance of features between the models. Additionally, I'll use `SHAP` to understand the importance of features in the models and to understand the predictions of the models. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import xgboost as xgb\n",
    "\n",
    "from house_price_utils import *\n",
    "\n",
    "setup_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, data_test = load_data()\n",
    "data = data[sorted(data)]\n",
    "X_raw = data.drop(columns=[\"SalePrice\"])\n",
    "y = data.SalePrice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import (\n",
    "    OrdinalEncoder,\n",
    ")\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = group_features(X_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_counts = {k: len(v) for k, v in features.items()}\n",
    "feature_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_preprocessor(features, continuous_strategy=\"mean\"):\n",
    "    continuous_transformer = make_pipeline(SimpleImputer(strategy=continuous_strategy))\n",
    "    categorical_transformer = make_pipeline(\n",
    "        OrdinalEncoder(\n",
    "            handle_unknown=\"use_encoded_value\",\n",
    "            unknown_value=-1,\n",
    "        ),\n",
    "    )\n",
    "    return make_column_transformer(\n",
    "        (continuous_transformer, features[\"continuous\"]),\n",
    "        (categorical_transformer, features[\"nominal\"] + features[\"ordinal\"]),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = make_preprocessor(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CV Split\n",
    "* Model comparison must be done using CV. I'll use `StratifiedKFold` to split the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t = preprocessor.fit_transform(X_raw)\n",
    "y_t = np.log1p(y).values\n",
    "y_quantiles = pd.qcut(y_t, q=5, labels=False)\n",
    "skfolds = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = skfolds.split(X_t, y_quantiles)\n",
    "(X_train, y_train), (X_val, y_val) = [(X_t[idx], y_t[idx]) for idx in next(cv)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_regressor = xgb.XGBRegressor(\n",
    "    objective=\"reg:squarederror\",\n",
    "    importance_type=\"gain\",\n",
    "    n_estimators=300,\n",
    "    learning_rate=0.05,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = xgb_regressor.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    eval_set=[(X_train, y_train), (X_val, y_val)],\n",
    "    verbose=25,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_importance(regressor, preprocessor, n=20):\n",
    "    return (\n",
    "        pd.DataFrame(\n",
    "            data=regressor.feature_importances_,\n",
    "            index=[c.split(\"__\")[1] for c in preprocessor.get_feature_names_out()],\n",
    "            columns=[\"importance\"],\n",
    "        )\n",
    "        .sort_values(by=\"importance\")\n",
    "        .iloc[-n:]\n",
    "        .plot(kind=\"barh\", figsize=(10, 7), title=\"Feature Importances\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plot_importance(xgb_regressor, preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = residual_plots(y_val, xgb_regressor.predict(X_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report(xgb_regressor, X_train, y_train, X_val, y_val, mean_squared_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_regressor = lgb.LGBMRegressor(\n",
    "    objective=\"regression\",\n",
    "    importance_type=\"gain\",\n",
    "    n_estimators=300,\n",
    "    learning_rate=0.05,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = lgbm_regressor.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plot_importance(lgbm_regressor, preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = residual_plots(y_val, lgbm_regressor.predict(X_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report(lgbm_regressor, X_train, y_train, X_val, y_val, mean_squared_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import catboost as cb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_regressor = cb.CatBoostRegressor(\n",
    "    loss_function=\"RMSE\",\n",
    "    n_estimators=300,\n",
    "    learning_rate=0.05,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = cb_regressor.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    eval_set=(X_val, y_val),\n",
    "    verbose=30,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plot_importance(cb_regressor, preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = residual_plots(y_val, cb_regressor.predict(X_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report(cb_regressor, X_train, y_train, X_val, y_val, mean_squared_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observations\n",
    "Model rankings are:\n",
    "1. CatBoost\n",
    "1. LightGBM\n",
    "1. XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimization with Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    params = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000),\n",
    "        \"depth\": trial.suggest_int(\"depth\", 2, 7),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 1),\n",
    "        \"l2_leaf_reg\": trial.suggest_float(\"l2_leaf_reg\", 0.001, 1),\n",
    "        \"verbose\": 0,\n",
    "    }\n",
    "    return -cross_val_score(\n",
    "        cb.CatBoostRegressor(**params),\n",
    "        X_t,\n",
    "        y_t,\n",
    "        cv=skfolds.split(X_t, y_quantiles),\n",
    "        scoring=\"neg_mean_squared_error\",\n",
    "    ).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=200, n_jobs=-1)\n",
    "print(study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = cb.CatBoostRegressor(**study.best_params)\n",
    "model.fit(X_t, y_t, verbose=0)\n",
    "print(f\"Full data MSE = {evaluate(model, X_t, y_t, mean_absolute_error):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = residual_plots(y_t, model.predict(X_t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame(\n",
    "    {\n",
    "        \"Id\": data_test.index,\n",
    "        \"SalePrice\": np.exp(model.predict(preprocessor.transform(data_test))),\n",
    "    }\n",
    ")\n",
    "output.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    import kaggle\n",
    "\n",
    "    result = kaggle.api.competition_submit(\n",
    "        \"submission.csv\",\n",
    "        f\"CatBoostRegressor optimized with Optuna\",\n",
    "        \"home-data-for-ml-course\",\n",
    "    )\n",
    "    print(result)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 111096,
     "sourceId": 10211,
     "sourceType": "competition"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
